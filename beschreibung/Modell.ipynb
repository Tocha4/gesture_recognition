{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell & Training\n",
    "Im Folgenden wird das unten abgebildete Modell aufgebaut und Trainiert. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://raw.githubusercontent.com/Tocha4/gesture_recognition/master/graph.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das geschriebene Modell und Hilfsfunktionen importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.chdir('../')\n",
    "from Modell import Modell\n",
    "from _Functions_gesture import load_gestures, batch_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden der Trainings- und Validierungs-Datensätze\n",
    "Das Training läuft Batchweise ab. Das Modell wird mit einer Batchgröße von 64 Bildern trainiert und mit 100 Bildern Validiert. Alle Trainingsbilder haben die Form (275,260) in Länge und Breite. Dieses Format erwart auch das Modell. Wenn eine andere Bildgröße und/oder Farbbilder verwendet werden sollen, muss im Skript layers.py in der Methode build_cnn die Eingangsgrößen angepasst werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_gestures(path='../../gestures/train')\n",
    "X_valid, y_valid = load_gestures(path='../../gestures/test')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Modell wird in einem Graphen erzeugt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Erste Schicht: Faltung_1\n",
      "1 <tf.Variable 'conv_1/_weights:0' shape=(5, 5, 1, 32) dtype=float32_ref>\n",
      "2 <tf.Variable 'conv_1/_biases:0' shape=(32,) dtype=float32_ref>\n",
      "3 Tensor(\"conv_1/Conv2D:0\", shape=(?, 271, 256, 32), dtype=float32)\n",
      "4 Tensor(\"conv_1/net_pre-activation:0\", shape=(?, 271, 256, 32), dtype=float32)\n",
      "5 Tensor(\"conv_1/activation:0\", shape=(?, 271, 256, 32), dtype=float32)\n",
      "\n",
      "Zweite Schicht: Faltung_2\n",
      "1 <tf.Variable 'conv_2/_weights:0' shape=(5, 5, 32, 48) dtype=float32_ref>\n",
      "2 <tf.Variable 'conv_2/_biases:0' shape=(48,) dtype=float32_ref>\n",
      "3 Tensor(\"conv_2/Conv2D:0\", shape=(?, 87, 82, 48), dtype=float32)\n",
      "4 Tensor(\"conv_2/net_pre-activation:0\", shape=(?, 87, 82, 48), dtype=float32)\n",
      "5 Tensor(\"conv_2/activation:0\", shape=(?, 87, 82, 48), dtype=float32)\n",
      "\n",
      "Zwischen Schicht: Faltung_3\n",
      "1 <tf.Variable 'conv_3/_weights:0' shape=(5, 5, 48, 64) dtype=float32_ref>\n",
      "2 <tf.Variable 'conv_3/_biases:0' shape=(64,) dtype=float32_ref>\n",
      "3 Tensor(\"conv_3/Conv2D:0\", shape=(?, 25, 24, 64), dtype=float32)\n",
      "4 Tensor(\"conv_3/net_pre-activation:0\", shape=(?, 25, 24, 64), dtype=float32)\n",
      "5 Tensor(\"conv_3/activation:0\", shape=(?, 25, 24, 64), dtype=float32)\n",
      "\n",
      "Dritte Schicht: Vollständig verknüpft\n",
      "Input_Units 9984\n",
      "weights: <tf.Variable 'fc_3/_weights:0' shape=(9984, 1000) dtype=float32_ref>\n",
      "biases: <tf.Variable 'fc_3/_biases:0' shape=(1000,) dtype=float32_ref>\n",
      "layer: Tensor(\"fc_3/MatMul:0\", shape=(?, 1000), dtype=float32)\n",
      "layer_2: Tensor(\"fc_3/activation:0\", shape=(?, 1000), dtype=float32)\n",
      "\n",
      "Vierte Schicht: Vollständig verknüpft -lineare aktivierung-\n",
      "Input_Units 1000\n",
      "weights: <tf.Variable 'fc_4/_weights:0' shape=(1000, 6) dtype=float32_ref>\n",
      "biases: <tf.Variable 'fc_4/_biases:0' shape=(6,) dtype=float32_ref>\n",
      "layer: Tensor(\"fc_4/MatMul:0\", shape=(?, 6), dtype=float32)\n",
      "WARNING:tensorflow:From /home/anton/Schreibtisch/DataScienceTraining/03_self.projects/gesture_recognition/skripts_gesture_recognition/layers.py:81: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Modell()\n",
    "learning_rate = 1e-4\n",
    "graph = model.build_cnn(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training in einer Session\n",
    "Zum Trainieren benötigt das Modell einen Trainingsdatensatz ```training_set``` aber nicht umbedingt einen Validierungsdatensatz. Wird ein neues Modell trainiert, muss die Variable ```initialize=True``` sein, andernfalls wird im Ordner ```../../model``` nach dem letzten trainiertem Modell gesucht und die Gewichte werden zum weiteren Training verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 Training LOSS: 373.882 Validierung ACCURACY:   0.850  Time: 0.21min\n",
      "Epoch 02 Training LOSS:  28.678 Validierung ACCURACY:   0.910  Time: 0.39min\n",
      "Epoch 03 Training LOSS:  11.877 Validierung ACCURACY:   0.950  Time: 0.57min\n",
      "Epoch 04 Training LOSS:   5.956 Validierung ACCURACY:   0.930  Time: 0.76min\n",
      "Epoch 05 Training LOSS:   4.125 Validierung ACCURACY:   0.940  Time: 0.94min\n",
      "Epoch 06 Training LOSS:   2.433 Validierung ACCURACY:   0.930  Time: 1.13min\n",
      "Epoch 07 Training LOSS:   2.358 Validierung ACCURACY:   0.950  Time: 1.32min\n",
      "Epoch 08 Training LOSS:   1.501 Validierung ACCURACY:   0.960  Time: 1.50min\n",
      "Epoch 09 Training LOSS:   0.989 Validierung ACCURACY:   0.970  Time: 1.69min\n",
      "Epoch 10 Training LOSS:   0.729 Validierung ACCURACY:   0.960  Time: 1.87min\n",
      "Epoch 11 Training LOSS:   1.517 Validierung ACCURACY:   0.940  Time: 2.06min\n",
      "Epoch 12 Training LOSS:   0.874 Validierung ACCURACY:   0.950  Time: 2.24min\n",
      "Epoch 13 Training LOSS:   0.663 Validierung ACCURACY:   0.970  Time: 2.43min\n",
      "Epoch 14 Training LOSS:   0.251 Validierung ACCURACY:   0.960  Time: 2.61min\n",
      "Epoch 15 Training LOSS:   0.323 Validierung ACCURACY:   0.950  Time: 2.79min\n",
      "Epoch 16 Training LOSS:   0.492 Validierung ACCURACY:   0.950  Time: 2.97min\n",
      "Epoch 17 Training LOSS:   1.387 Validierung ACCURACY:   0.980  Time: 3.16min\n",
      "Epoch 18 Training LOSS:   2.331 Validierung ACCURACY:   0.950  Time: 3.34min\n",
      "Epoch 19 Training LOSS:   1.643 Validierung ACCURACY:   0.970  Time: 3.52min\n",
      "Epoch 20 Training LOSS:   0.976 Validierung ACCURACY:   0.950  Time: 3.71min\n",
      "Modell speichern in ../model/2018081906531534654417\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:        \n",
    "    model.train(sess, \n",
    "                training_set=(X_train,y_train),\n",
    "                validation_set=(X_valid, y_valid), \n",
    "                initialize=True,\n",
    "                random_seed=123, \n",
    "                epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beobachtungen\n",
    "Das oben dargestellte Training habe ich auf einer GPU (Nvidia GForce 1070i) durchgeführt. Damit sind in kürzester Zeit hohe Genauigkeiten möglich. Diese hohen Genauigkeiten bei den wenigen Epochs lassen auf einen einfachen Datensatz dueten. Um das Modell zu verbesseren und stabiler zu machen, sollten Gesten bei unterschiedlichen Hintergründen aufgenommen werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
